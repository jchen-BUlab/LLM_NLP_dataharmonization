{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36cc4d70-2709-4505-9686-e2a6d295206b",
   "metadata": {},
   "source": [
    "## A Large Language Model-based tool to facilitate data harmonization: feature analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46b7dd4-45d0-432b-84d5-e56cf23c541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#****************************************\n",
    "# MIT License\n",
    "# Copyright (c) 2025 Zexu Li, Jinying Chen\n",
    "#  \n",
    "# author(s): Zexu Li, Jinying Chen, Boston University Chobanian & Avedisian School of Medicine\n",
    "# date: 2025-7-7\n",
    "# ver: 1.0\n",
    "# \n",
    "# This code was written to support data analysis for the Data Harmonization Using Natural Language \n",
    "# Processing (NLP harmonization) project and the 2025 paper published in PLOS One.\n",
    "# The code is for research use only, and is provided as it is.\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f13993",
   "metadata": {},
   "source": [
    "## Including permutation importance calculation, paired t-test on 50 trails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84151c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score,f1_score\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc, make_scorer, recall_score, accuracy_score, precision_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import ast\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698eb7fa-1dd9-4355-8750-da0cf35538d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load input data for feature analysis\n",
    "datadir = \"[path to input data]\"\n",
    "Final_df_ML = pd.read_csv(datadir+'ML_dataset_021825_v3.csv')\n",
    "\n",
    "# path to ML outputs from 50 trials\n",
    "# used to extract train/test split information by the function main_perm\n",
    "folder_path = datadir + \"/02232025res_v2/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d532c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_difference_and_ci(column1, column2, confidence_level=0.95):\n",
    "    \"\"\"\n",
    "    Calculate the mean difference and 95% confidence interval for the difference in means\n",
    "    between two columns of values.\n",
    "    \n",
    "    Parameters:\n",
    "    column1 (list or np.array): First column of values\n",
    "    column2 (list or np.array): Second column of values\n",
    "    confidence_level (float): Confidence level for the interval (default is 0.95 for 95% CI)\n",
    "    \n",
    "    Returns:\n",
    "    tuple: mean difference, (lower bound of CI, upper bound of CI)\n",
    "    \"\"\"\n",
    "    # Convert the lists to numpy arrays\n",
    "    column1 = np.array(column1)\n",
    "    column2 = np.array(column2)\n",
    "\n",
    "    # Calculate the means\n",
    "    mean1 = np.mean(column1)\n",
    "    mean2 = np.mean(column2)\n",
    "\n",
    "    # Calculate the standard deviations\n",
    "    std1 = np.std(column1, ddof=1)  # Using ddof=1 to get the sample standard deviation\n",
    "    std2 = np.std(column2, ddof=1)\n",
    "\n",
    "    # Calculate the sample sizes\n",
    "    n1 = len(column1)\n",
    "    n2 = len(column2)\n",
    "\n",
    "    # Calculate the standard error for the difference in means\n",
    "    sem = np.sqrt((std1**2 / n1) + (std2**2 / n2))\n",
    "\n",
    "    # Calculate the degrees of freedom\n",
    "    df = n1 + n2 - 2\n",
    "\n",
    "    # Calculate the critical value from the t-distribution for the given confidence interval\n",
    "    critical_value = stats.t.ppf((1 + confidence_level) / 2, df)\n",
    "\n",
    "    # Calculate the margin of error\n",
    "    margin_of_error = critical_value * sem\n",
    "\n",
    "    # Calculate the confidence interval\n",
    "    mean_difference = mean1 - mean2\n",
    "    confidence_interval = (mean_difference - margin_of_error, mean_difference + margin_of_error)\n",
    "\n",
    "    return mean_difference, confidence_interval\n",
    "\n",
    "\n",
    "def extract_between_phrases(file_path, start_phrase, end_phrase):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    start_index = -1\n",
    "    end_index = -1\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.startswith(start_phrase):\n",
    "            start_index = i\n",
    "        if end_phrase in line:\n",
    "            end_index = i\n",
    "            break\n",
    "    \n",
    "    if start_index != -1 and end_index != -1:\n",
    "        return ''.join([line.strip() for line in lines[start_index:end_index+1]])\n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "def extract_train_sources(text):\n",
    "    # Use regex to find the 'Train Source' and 'Test Source' parts\n",
    "    train_match = re.search(r\"Train Source:\\[(.*?)\\], Test Source\", text)\n",
    "    \n",
    "    if train_match:\n",
    "        train_sources = train_match.group(1)\n",
    "        # Split the sources by separating them with ' ' and remove any empty strings\n",
    "        train_sources_list = [source for source in train_sources.split(\"'\") if source.strip()]\n",
    "        if ', ' in train_sources_list:\n",
    "            train_sources_list = [item for item in train_sources_list if item != ', ']\n",
    "        return train_sources_list\n",
    "    else:\n",
    "        return \"\"\n",
    "    \n",
    "    \n",
    "def extract_lines(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        relevant_lines = [line.strip() for line in lines if line.strip().startswith('Best Grid')]\n",
    "        return relevant_lines\n",
    "    \n",
    "def select_true_and_false(df_source):\n",
    "    true_values = df_source[df_source['Mapping_result'] == 1]\n",
    "    false_values = df_source[df_source['Mapping_result'] == 0].sample(n=200*len(true_values), random_state=42)\n",
    "    return pd.concat([true_values, false_values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e834e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main function to generate the permutation result\n",
    "#Permutation importance\n",
    "\n",
    "def main_perm(Final_df_ML):\n",
    "    result = {}\n",
    "    #result_SKIpermu = {}\n",
    "    file_num = 0\n",
    "    ALL_permu_importance_dict30 = {}\n",
    "    ALL_permu_mean_importance_dict30 = {}\n",
    "    ALL_permu_importance_dict20 = {}\n",
    "    ALL_permu_mean_importance_dict20 = {}\n",
    "    ALL_permu_importance_dict10 = {}\n",
    "    ALL_permu_mean_importance_dict10 = {}\n",
    "    ALL_permu_importance_dict5 = {}\n",
    "    ALL_permu_mean_importance_dict5 = {}\n",
    "    ALL_permu_importance_dictMRR = {}\n",
    "    ALL_permu_mean_importance_dictMRR = {}\n",
    "    \n",
    "    test_res_full = {}\n",
    "    \n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_num+=1\n",
    "        if file_name.endswith('.txt') and file_name.startswith('output'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            text = extract_between_phrases(file_path,'Train Source','Test Source')\n",
    "            train_sources = extract_train_sources(text)\n",
    "            #print(train_source)\n",
    "            \n",
    "            relevant_lines = extract_lines(file_path)\n",
    "            print(f\"Numbers extracted from {file_name}:\")\n",
    "            \n",
    "            grids = {}\n",
    "            i = 0\n",
    "            for line in relevant_lines:\n",
    "                #print(line)\n",
    "                if line.startswith('Best Grid'):\n",
    "                    i+=1\n",
    "                    Grid = re.search(r\"Best Grid:\\{(.*?)\\}, unique_HR\", line)\n",
    "                    grids[i] = Grid.group(1)\n",
    "    \n",
    "            train = Final_df_ML[Final_df_ML['Source'].isin(train_sources)]\n",
    "            test = Final_df_ML[~Final_df_ML['Source'].isin(train_sources)]\n",
    "            test_copy = test.copy()\n",
    "        \n",
    "        \n",
    "            train_dataonly = train.groupby('Source').apply(select_true_and_false).reset_index(drop=True)\n",
    "            \n",
    "            train_dataonly = train_dataonly[['miniLM_on_label', 'e5_on_label', 'mpnet_on_label', 'fuzzy_on_label','biolord_on_label',\n",
    "                           'miniLM_on_label_key', 'e5_on_label_key', 'mpnet_on_label_key', 'fuzzy_on_label_key','biolord_on_label_key',\n",
    "                            'miniLM_on_sheet', 'e5_on_sheet', 'mpnet_on_sheet', 'fuzzy_on_sheet','biolord_on_sheet',\n",
    "                               'Label_len_EU','deriv_info_null_EU','deriv_info_len_EU', 'deriv_info_null_JP','deriv_info_len_JP',\n",
    "                               'Label_len_JP','Mapping_result']]\n",
    "            test_dataonly = test[['miniLM_on_label', 'e5_on_label', 'mpnet_on_label', 'fuzzy_on_label','biolord_on_label',\n",
    "                           'miniLM_on_label_key', 'e5_on_label_key', 'mpnet_on_label_key', 'fuzzy_on_label_key','biolord_on_label_key',\n",
    "                            'miniLM_on_sheet', 'e5_on_sheet', 'mpnet_on_sheet', 'fuzzy_on_sheet','biolord_on_sheet',\n",
    "                               'Label_len_EU','deriv_info_null_EU','deriv_info_len_EU', 'deriv_info_null_JP','deriv_info_len_JP',\n",
    "                               'Label_len_JP','Mapping_result']]\n",
    "            \n",
    "            X_train = train_dataonly.drop('Mapping_result',axis = 1)\n",
    "            y_train = train_dataonly[['Mapping_result']].values.ravel()\n",
    "            X_test = test_dataonly.drop('Mapping_result',axis = 1)\n",
    "            y_test = test_dataonly[['Mapping_result']].values.ravel()\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            rf = RandomForestClassifier(random_state=42)\n",
    "            print('Full model complete')\n",
    "            dict_string = \"{\" + grids.get(3) + \"}\"\n",
    "            # Convert the string to a dictionary\n",
    "            g = ast.literal_eval(dict_string)\n",
    "            rf.set_params(**g)\n",
    "            rf.fit(X_train,y_train)\n",
    "            \n",
    "            \n",
    "            feature_importances = rf.feature_importances_\n",
    "            # Create a DataFrame to hold feature names and their importance scores\n",
    "            features_df = pd.DataFrame({\n",
    "                'Feature': X_train.columns,\n",
    "                'Importance': feature_importances\n",
    "            })\n",
    "\n",
    "            # Sort the DataFrame by importance scores in descending order\n",
    "            ranked_features_df = features_df.sort_values(by='Importance', ascending=False)\n",
    "            result[file_num] = ranked_features_df\n",
    "            \n",
    "            \n",
    "            #full\n",
    "            y_pred = rf.predict(X_test)\n",
    "            y_pred_proba = rf.predict_proba(X_test)[:, 1]\n",
    "            test_copy['probability'] = y_pred_proba\n",
    "            test_copy['rank'] = test_copy.groupby('Source')['probability'].rank(ascending=False)\n",
    "            positive = test_copy[test_copy['Mapping_result'] == 1]\n",
    "            unique_positive = positive.groupby('Source')['rank'].min().reset_index()\n",
    "            top_30_HR_unique = len(unique_positive[unique_positive['rank'] <=30])/len(unique_positive)\n",
    "            top_20_HR_unique = len(unique_positive[unique_positive['rank'] <=20])/len(unique_positive)\n",
    "            top_10_HR_unique = len(unique_positive[unique_positive['rank'] <=10])/len(unique_positive)\n",
    "            top_5_HR_unique = len(unique_positive[unique_positive['rank'] <=5])/len(unique_positive)\n",
    "            max_ranks = positive.groupby('Source')['rank'].idxmin()\n",
    "            result_df = positive.loc[max_ranks]\n",
    "            result_df['MRR'] = 1/result_df['rank']\n",
    "            MRR = sum(result_df['MRR'])/ len(result_df)\n",
    "            test_res_full[file_num] = [top_30_HR_unique,top_20_HR_unique,top_10_HR_unique,top_5_HR_unique,MRR]\n",
    "            \n",
    "            \n",
    "            \n",
    "            permu_importance_dict30 = {}\n",
    "            permu_mean_importance_dict30 = {}\n",
    "            permu_importance_dict20 = {}\n",
    "            permu_mean_importance_dict20 = {}\n",
    "            permu_importance_dict10 = {}\n",
    "            permu_mean_importance_dict10 = {}\n",
    "            permu_importance_dict5 = {}\n",
    "            permu_mean_importance_dict5 = {}\n",
    "            permu_importance_dictMRR = {}\n",
    "            permu_mean_importance_dictMRR = {}\n",
    "            #Permutation imp from algorithm\n",
    "            for columns_name in X_test.columns:\n",
    "                print(columns_name)\n",
    "                top_30_HR_permu_list = []\n",
    "                top_20_HR_permu_list = []\n",
    "                top_10_HR_permu_list = []\n",
    "                top_5_HR_permu_list = []\n",
    "                MRR_permu_list = []\n",
    "                for repeat_num in range(20):\n",
    "                    test_for_permu = test.copy()\n",
    "                    X_test_shuffle = X_test.copy()#deep copy\n",
    "                    X_test_shuffle[columns_name] = np.random.RandomState(seed=repeat_num).permutation(X_test_shuffle[columns_name].values)# np.random.RandomState(seed=repeat_num).permutation(10)\n",
    "                    y_pred_proba_permu = rf.predict_proba(X_test_shuffle)[:, 1]\n",
    "                    test_for_permu['probability'] = y_pred_proba_permu\n",
    "                    test_for_permu['rank'] = test_for_permu.groupby('Source')['probability'].rank(ascending=False)\n",
    "                    positive_permu = test_for_permu[test_for_permu['Mapping_result'] == 1]\n",
    "                    unique_positive_permu = positive_permu.groupby('Source')['rank'].min().reset_index()\n",
    "                    top_30_HR_unique_permu = len(unique_positive_permu[unique_positive_permu['rank'] <=30])/len(unique_positive_permu)\n",
    "                    top_20_HR_unique_permu = len(unique_positive_permu[unique_positive_permu['rank'] <=20])/len(unique_positive_permu)\n",
    "                    top_10_HR_unique_permu = len(unique_positive_permu[unique_positive_permu['rank'] <=10])/len(unique_positive_permu)\n",
    "                    top_5_HR_unique_permu = len(unique_positive_permu[unique_positive_permu['rank'] <=5])/len(unique_positive_permu)\n",
    "                    top_30_HR_permu_list.append(top_30_HR_unique - top_30_HR_unique_permu)\n",
    "                    top_20_HR_permu_list.append(top_20_HR_unique - top_20_HR_unique_permu)\n",
    "                    top_10_HR_permu_list.append(top_10_HR_unique - top_10_HR_unique_permu)\n",
    "                    top_5_HR_permu_list.append(top_5_HR_unique - top_5_HR_unique_permu)\n",
    "                    \n",
    "                    max_ranks_permu = positive_permu.groupby('Source')['rank'].idxmin()\n",
    "                    result_df_permu = positive_permu.loc[max_ranks_permu]\n",
    "                    result_df_permu['MRR'] = 1/result_df_permu['rank']\n",
    "                    MRR_permu = sum(result_df_permu['MRR'])/ len(result_df_permu)\n",
    "                    MRR_permu_list.append(MRR - MRR_permu)\n",
    "                permu_importance_dict30[columns_name] = top_30_HR_permu_list\n",
    "                permu_mean_importance_dict30[columns_name] = np.mean(top_30_HR_permu_list)\n",
    "                permu_importance_dict20[columns_name] = top_20_HR_permu_list\n",
    "                permu_mean_importance_dict20[columns_name] = np.mean(top_20_HR_permu_list)\n",
    "                permu_importance_dict10[columns_name] = top_10_HR_permu_list\n",
    "                permu_mean_importance_dict10[columns_name] = np.mean(top_10_HR_permu_list)\n",
    "                permu_importance_dict5[columns_name] = top_5_HR_permu_list\n",
    "                permu_mean_importance_dict5[columns_name] = np.mean(top_5_HR_permu_list)\n",
    "                permu_importance_dictMRR[columns_name] = MRR_permu_list\n",
    "                permu_mean_importance_dictMRR[columns_name] = np.mean(MRR_permu_list)\n",
    "            ALL_permu_importance_dict30[file_num] = permu_importance_dict30\n",
    "            ALL_permu_mean_importance_dict30[file_num] = permu_mean_importance_dict30\n",
    "            ALL_permu_importance_dict20[file_num] = permu_importance_dict20\n",
    "            ALL_permu_mean_importance_dict20[file_num] = permu_mean_importance_dict20\n",
    "            ALL_permu_importance_dict10[file_num] = permu_importance_dict10\n",
    "            ALL_permu_mean_importance_dict10[file_num] = permu_mean_importance_dict10\n",
    "            ALL_permu_importance_dict5[file_num] = permu_importance_dict5\n",
    "            ALL_permu_mean_importance_dict5[file_num] = permu_mean_importance_dict5\n",
    "            ALL_permu_importance_dictMRR[file_num] = permu_importance_dictMRR\n",
    "            ALL_permu_mean_importance_dictMRR[file_num] = permu_mean_importance_dictMRR\n",
    "           \n",
    "            \n",
    "    return result,ALL_permu_importance_dict30,ALL_permu_mean_importance_dict30,ALL_permu_importance_dict20,ALL_permu_mean_importance_dict20,ALL_permu_importance_dict10,ALL_permu_mean_importance_dict10,ALL_permu_importance_dict5,ALL_permu_mean_importance_dict5 ,ALL_permu_importance_dictMRR,ALL_permu_mean_importance_dictMRR,test_res_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0833d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_across_dicts(dict_list):\n",
    "    \"\"\"\n",
    "    Calculate the mean value for each key across multiple dictionaries.\n",
    "\n",
    "    Parameters:\n",
    "    dict_list (list of dict): A list of dictionaries with the same keys.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary with keys and their corresponding mean values.\n",
    "    \"\"\"\n",
    "    # Initialize sum and count dictionaries\n",
    "    sums = {}\n",
    "    counts = {}\n",
    "\n",
    "    # Iterate through each dictionary\n",
    "    for d in dict_list.values():\n",
    "        for key, value in d.items():\n",
    "            if key not in sums:\n",
    "                sums[key] = 0\n",
    "                counts[key] = 0\n",
    "            sums[key] += value\n",
    "            counts[key] += 1\n",
    "\n",
    "    # Calculate the mean for each key\n",
    "    means = {key: sums[key] / counts[key] for key in sums}\n",
    "    df = pd.DataFrame(list(means.items()), columns=['key', 'mean'])\n",
    "\n",
    "    # Add a rank column\n",
    "    df['rank'] = df['mean'].rank(ascending=False, method='min')\n",
    "\n",
    "    # Sort the DataFrame by rank\n",
    "    df = df.sort_values(by='rank')\n",
    "\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def rank_feq_5(row):\n",
    "    n = 0\n",
    "    for i in rank_list:\n",
    "        if row[i]<=5: \n",
    "            n+=1\n",
    "    return n/50\n",
    "\n",
    "def rank_feq_10(row):\n",
    "    n = 0\n",
    "    for i in rank_list:\n",
    "        if row[i]<=10: \n",
    "            n+=1\n",
    "    return n/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac858d24-17b9-4def-b358-3c3d1521e63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result,ALL_permu_importance_dict30,ALL_permu_mean_importance_dict30,ALL_permu_importance_dict20,ALL_permu_mean_importance_dict20,ALL_permu_importance_dict10,ALL_permu_mean_importance_dict10,ALL_permu_importance_dict5,ALL_permu_mean_importance_dict5 ,ALL_permu_importance_dictMRR,ALL_permu_mean_importance_dictMRR,test_res_full = main_perm(Final_df_ML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb11dfe-c60a-4591-be16-fb88fe9c4a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(datadir + 'ALL_permu_mean_importance_dict5.pkl', 'wb') as f:\n",
    "    pickle.dump(ALL_permu_mean_importance_dict5, f)\n",
    "\n",
    "with open(datadir + 'ALL_permu_mean_importance_dict10.pkl', 'wb') as f:\n",
    "    pickle.dump(ALL_permu_mean_importance_dict10, f)\n",
    "\n",
    "with open(datadir + 'ALL_permu_mean_importance_dictMRR.pkl', 'wb') as f:\n",
    "    pickle.dump(ALL_permu_mean_importance_dictMRR, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675e2169",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process the hr-5 permutation result (average over 20 random seed)\n",
    "import pickle\n",
    "file_path = datadir + 'ALL_permu_mean_importance_dict5.pkl'\n",
    "with open(file_path, 'rb') as file:\n",
    "    ALL_permu_mean_importance_dict5 = pickle.load(file)\n",
    "HR5 = calculate_mean_across_dicts(ALL_permu_mean_importance_dict5)\n",
    "\n",
    "HR5_avg_df = pd.DataFrame.from_dict(ALL_permu_mean_importance_dict5, orient='index').T\n",
    "for i in HR5_avg_df.columns:\n",
    "    rank_name = str(i)+'_rank'\n",
    "    HR5_avg_df[rank_name] = HR5_avg_df[i].rank(ascending= False,method='min')\n",
    "    \n",
    "\n",
    "importance_list = []\n",
    "rank_list = []\n",
    "for i in range(1,51):\n",
    "    imp = i\n",
    "    rank =  str(i) +'_rank'\n",
    "    importance_list.append(imp)\n",
    "    rank_list.append(rank)\n",
    "HR5_avg_df['Avg_importance'] = HR5_avg_df[importance_list].sum(axis = 1)/50\n",
    "HR5_avg_df['Avg_rank'] = HR5_avg_df[rank_list].sum(axis = 1)/50\n",
    "\n",
    "\n",
    "\n",
    "HR5_avg_df['top_10_feq'] = HR5_avg_df.apply(rank_feq_10,axis = 1)\n",
    "HR5_avg_df['top_5_feq'] = HR5_avg_df.apply(rank_feq_5,axis = 1)\n",
    "#HR5_avg_df.to_csv('HR5_avg_df.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4da4aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "HR5_res = HR5_avg_df[['Avg_importance','Avg_rank','top_10_feq','top_5_feq']].rename(columns=lambda x: f\"{x}{'_HR5'}\")\n",
    "HR5_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c48005",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hr10 result\n",
    "import pickle\n",
    "file_path = datadir + 'ALL_permu_mean_importance_dict10.pkl'\n",
    "with open(file_path, 'rb') as file:\n",
    "    ALL_permu_mean_importance_dict10 = pickle.load(file)\n",
    "HR10 = calculate_mean_across_dicts(ALL_permu_mean_importance_dict10)\n",
    "\n",
    "HR10_avg_df = pd.DataFrame.from_dict(ALL_permu_mean_importance_dict10, orient='index').T\n",
    "for i in HR10_avg_df.columns:\n",
    "    rank_name = str(i)+'_rank'\n",
    "    HR10_avg_df[rank_name] = HR10_avg_df[i].rank(ascending= False,method='min')\n",
    "    \n",
    "\n",
    "importance_list = []\n",
    "rank_list = []\n",
    "for i in range(1,51):\n",
    "    imp = i\n",
    "    rank =  str(i) +'_rank'\n",
    "    importance_list.append(imp)\n",
    "    rank_list.append(rank)\n",
    "HR10_avg_df['Avg_importance'] = HR10_avg_df[importance_list].sum(axis = 1)/50\n",
    "HR10_avg_df['Avg_rank'] = HR10_avg_df[rank_list].sum(axis = 1)/50\n",
    "\n",
    "def rank_feq_5(row):\n",
    "    n = 0\n",
    "    for i in rank_list:\n",
    "        if row[i]<=5: \n",
    "            n+=1\n",
    "    return n/50\n",
    "\n",
    "def rank_feq_10(row):\n",
    "    n = 0\n",
    "    for i in rank_list:\n",
    "        if row[i]<=10: \n",
    "            n+=1\n",
    "    return n/50\n",
    "\n",
    "HR10_avg_df['top_10_feq'] = HR10_avg_df.apply(rank_feq_10,axis = 1)\n",
    "HR10_avg_df['top_5_feq'] = HR10_avg_df.apply(rank_feq_5,axis = 1)\n",
    "#HR10_avg_df.to_csv('HR10_avg_df.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d4fb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MRR result\n",
    "import pickle\n",
    "file_path = datadir + 'ALL_permu_mean_importance_dictMRR.pkl'\n",
    "with open(file_path, 'rb') as file:\n",
    "    ALL_permu_mean_importance_dictMRR = pickle.load(file)\n",
    "MRR = calculate_mean_across_dicts(ALL_permu_mean_importance_dictMRR)\n",
    "\n",
    "MRR_avg_df = pd.DataFrame.from_dict(ALL_permu_mean_importance_dictMRR, orient='index').T\n",
    "for i in MRR_avg_df.columns:\n",
    "    rank_name = str(i)+'_rank'\n",
    "    MRR_avg_df[rank_name] = MRR_avg_df[i].rank(ascending= False,method='min')\n",
    "    \n",
    "\n",
    "importance_list = []\n",
    "rank_list = []\n",
    "for i in range(1,51):\n",
    "    imp = i\n",
    "    rank =  str(i) +'_rank'\n",
    "    importance_list.append(imp)\n",
    "    rank_list.append(rank)\n",
    "MRR_avg_df['Avg_importance'] = MRR_avg_df[importance_list].sum(axis = 1)/50\n",
    "MRR_avg_df['Avg_rank'] = MRR_avg_df[rank_list].sum(axis = 1)/50\n",
    "\n",
    "def rank_feq_5(row):\n",
    "    n = 0\n",
    "    for i in rank_list:\n",
    "        if row[i]<=5: \n",
    "            n+=1\n",
    "    return n/50\n",
    "\n",
    "def rank_feq_10(row):\n",
    "    n = 0\n",
    "    for i in rank_list:\n",
    "        if row[i]<=10: \n",
    "            n+=1\n",
    "    return n/50\n",
    "\n",
    "MRR_avg_df['top_10_feq'] = MRR_avg_df.apply(rank_feq_10,axis = 1)\n",
    "MRR_avg_df['top_5_feq'] = MRR_avg_df.apply(rank_feq_5,axis = 1)\n",
    "#MRR_avg_df.to_csv('MRR_avg_df.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2810a799",
   "metadata": {},
   "outputs": [],
   "source": [
    "MRR_res = MRR_avg_df[['Avg_importance','Avg_rank','top_10_feq','top_5_feq']].rename(columns=lambda x: f\"{x}{'_MRR'}\")\n",
    "HR10_res = HR10_avg_df[['Avg_importance','Avg_rank','top_10_feq','top_5_feq']].rename(columns=lambda x: f\"{x}{'_HR10'}\")\n",
    "HR5_res = HR5_avg_df[['Avg_importance','Avg_rank','top_10_feq','top_5_feq']].rename(columns=lambda x: f\"{x}{'_HR5'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
