{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa162fe4-2bcc-469d-904d-55d1f402a30c",
   "metadata": {},
   "source": [
    "## A Large Language Model-based tool to facilitate data harmonization: individual NLP models used to align variables across cohort studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52167c5d-80b5-41e0-8d23-ed63b5601841",
   "metadata": {},
   "outputs": [],
   "source": [
    "#****************************************\n",
    "# MIT License\n",
    "# Copyright (c) 2025 Zexu Li, Suraj P. Prabhu, Jinying Chen\n",
    "#  \n",
    "# author(s): Zexu Li, Suraj P. Prabhu, Jinying Chen, Boston University Chobanian & Avedisian School of Medicine\n",
    "# date: 2025-7-7\n",
    "# ver: 1.0\n",
    "# \n",
    "# This code was written to support data analysis for the Data Harmonization Using Natural Language \n",
    "# Processing (NLP harmonization) project and the 2025 paper published in PLOS One.\n",
    "# The code is for research use only, and is provided as it is.\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d585a840-ed2d-4f5f-9b54-951d02e33132",
   "metadata": {},
   "source": [
    "## Section 1. packages importing and installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124a0842-d633-4a6d-983a-1024128ef8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel \n",
    "import torch\n",
    "#pip install -U sentence-transformers\n",
    "#pip install transformers==4.18.0\n",
    "#conda update --all\n",
    "#conda install xlrd\n",
    "from scipy.spatial.distance import cosine\n",
    "import matplotlib.pyplot as plt\n",
    "#pip install fuzzywuzzy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "#pip install python-Levenshtein\n",
    "import numpy as np\n",
    "from sentence_transformers import CrossEncoder\n",
    "import itertools\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7279ce-b0c7-44dd-b41a-d7a70f8a373d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library for Fuzzy match\n",
    "#!pip install fuzzywuzzy\n",
    "#!pip install python-Levenshtein\n",
    "#!pip install --upgrade pip\n",
    "\n",
    "import fuzzywuzzy\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70e9c36-32d2-4c0b-9307-97631233800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install keybert\n",
    "from keybert import KeyBERT\n",
    "kw_model = KeyBERT()\n",
    "\n",
    "#!pip install adjustText\n",
    "from adjustText import adjust_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f8d4ae",
   "metadata": {},
   "source": [
    "## Section 2. Data Dictionary and Truth Map Reading (Input files, example can be found in example_inputs folder)\n",
    "The aim of the example is to find alignment for each EU variable in all JP variables. Source: EU_all, Target: JP_all, Truth_table: Ground truth for evaluation.\n",
    "1. 'JP_all_ML1111.csv' contain all the variables from JP\n",
    "2. 'EU_all_ML1111.csv' contain all the variables from EU\n",
    "3. 'Truth_table_1115.csv' contain truth alignments. This is manual alignments for methods evaluation purpose, which will not exist for other datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4beb8c2-aaa6-46b0-9d1a-6193dcc9ff81",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"[path to input data]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23af271",
   "metadata": {},
   "outputs": [],
   "source": [
    "JP_all = pd.read_csv(datadir + 'JP_all_ML1111.csv') #Target Variables with columns: Sheet_Def\tDomain\tVariable\tLabel\tType\tCodes\tRule for derivation\n",
    "EU_all = pd.read_csv(datadir + 'EU_all_ML1111.csv') #Source Variables with columns: Sheet_Def\tADS Name\tVariable\tLabel\tDefinition (derived Vars)\tType\tCRF Question\tLabel_CRF\n",
    "Truth_table = pd.read_csv(datadir + 'Truth_table_1115.csv') #Truth Map: with columns: Variable_EU\tVariable_JP\n",
    "\n",
    "top_n = len(JP_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc2045e",
   "metadata": {},
   "source": [
    "## Section 3. Data dictionary Data preprocessing\n",
    "1. Add key words extraction on rule for derivation.\n",
    "2. Add derived measurements based on data dictionary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf545f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_len(x):\n",
    "    '''return the number of words in sentence after replacing punctuation with space'''\n",
    "    if pd.isnull(x):\n",
    "        return 0\n",
    "    else:\n",
    "        x = str(x)\n",
    "        translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation)) #replace punctuation with space\n",
    "        x = x.translate(translator)\n",
    "        words = x.split() #split sentence on space\n",
    "        word_count = len(words)\n",
    "        return word_count\n",
    "    \n",
    "def validate_null(x):\n",
    "    '''Validate whether the cell is null'''\n",
    "    if pd.isnull(x):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def combine_JP_deriv_info(x):\n",
    "    '''Combine multiple columns to form the Rule for derivation column'''\n",
    "    if pd.isnull(x['Codes']) and pd.isnull(x['Rule for derivation']):\n",
    "        return np.nan\n",
    "    elif pd.isnull(x['Codes']) and pd.notnull(x['Rule for derivation']):\n",
    "        return str(x['Rule for derivation'])\n",
    "    elif pd.notnull(x['Codes']) and pd.isnull(x['Rule for derivation']):\n",
    "        return str(x['Codes'])\n",
    "    else:\n",
    "        if x['Codes'] == x['Rule for derivation'] :\n",
    "            return str(x['Rule for derivation'])\n",
    "        else:\n",
    "            return str(x['Codes']) + '; ' + str(x['Rule for derivation'])\n",
    "\n",
    "def extract_words(x):\n",
    "    '''Extract key words from rule for deriviation column if it has more than 20 words, using keybert'''\n",
    "    if x['deriv_info_len'] > 20: #limit of words to begain key words extraction\n",
    "        keyword_list = kw_model.extract_keywords(x['deriv_info'],keyphrase_ngram_range=(5, 5), stop_words='english', use_maxsum=True, nr_candidates=20, top_n=3)\n",
    "        new_str = ''\n",
    "        for i in keyword_list:\n",
    "            new_str = new_str +i[0]  +'. '\n",
    "        return new_str\n",
    "    else:\n",
    "        return x['deriv_info']\n",
    "\n",
    "def label_keywords_combine(x):\n",
    "    '''Combine label with deriv_info_key_words into one sentence (in case a variable don't have deriv_info_key_words as null)'''\n",
    "    if pd.isnull(x['deriv_info_key_words']):\n",
    "        return x['Label']\n",
    "    else:\n",
    "        if x['Label'] == x['deriv_info_key_words']:\n",
    "            return x['Label']\n",
    "        else:\n",
    "            return str(x['Label']) + '. ' + str(x['deriv_info_key_words'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29c3d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JP_all preprocessing \n",
    "JP_all = JP_all.drop_duplicates(subset=['Variable']) #Drop Duplicate columns\n",
    "JP_all['deriv_info'] = JP_all.apply(combine_JP_deriv_info,axis = 1)  #Re-organize deriviation rule \n",
    "JP_all['deriv_info_null'] = JP_all['deriv_info'].apply(validate_null) #Check whether deriviation rule is null or not\n",
    "JP_all['deriv_info_len'] = JP_all['deriv_info'].apply(get_len) #Return length of deriviation rule\n",
    "JP_all['Label_len'] = JP_all['Label'].apply(get_len) #Return length of label rule\n",
    "JP_all['deriv_info_key_words'] = JP_all.apply(extract_words,axis = 1) #extract key words if the rule for deriviation is too long\n",
    "JP_all['label_keywords'] = JP_all.apply(label_keywords_combine, axis = 1) #Final version of deriviation rule info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f9a4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EU_all preprocessing \n",
    "EU_all = EU_all.drop_duplicates(subset=['Variable'])\n",
    "EU_all['deriv_info'] = EU_all['Definition (derived Vars)']  #Re-organize deriviation rule\n",
    "EU_all['deriv_info_null'] = EU_all['deriv_info'].apply(validate_null)  #Check whether deriviation rule is null or not\n",
    "EU_all['deriv_info_len'] = EU_all['deriv_info'].apply(get_len) #Return length of deriviation rule\n",
    "EU_all['Label_len'] = EU_all['Label'].apply(get_len) #Return length of label rule\n",
    "EU_all['deriv_info_key_words'] = EU_all.apply(extract_words,axis = 1) #extract key words if the rule for deriviation is too long\n",
    "EU_all['label_keywords'] = EU_all.apply(label_keywords_combine, axis = 1) #Final version of deriviation rule info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12207b4",
   "metadata": {},
   "source": [
    "## Section 4. Fuzzy Match Class\n",
    "Fuzzy match algorithms that used fuzzywuzzy package. Get similarity between two strings using Fuzzywuzzy algorithms\n",
    "THe algorithm take 5 parameters as input: source_str_list,target_str_list,top_n,source_var_names,target_var_names\n",
    "1. source_str_list: In this example, EU label/sheet_def/deriv_info strings in a list\n",
    "2. source_var_names: In this example, EU variable name that corresponding to EU label/sheet_def/deriv_info strings in a list\n",
    "3. target_str_list: In this example, JP label/sheet_def/deriv_info strings in a list\n",
    "4. target_var_names: In this example, JP variable name that corresponding to EU label/sheet_def/deriv_info strings in a list\n",
    "5. top_n: Number of top Similar variables you want to left in reuslt datasets. Must be smaller or equal to len(str_list2)\n",
    "\n",
    "Return format:Target_Variable, Similar_var_#, Similar_value_#\n",
    "1. Target_Variable: Variable names from the source data dictionary. Duplicate variable are dropped, variables with not label or definition are dropped.\n",
    "2. Similar_var_#: Variable from the target data dictionary. Ranked based on the similarity score between target Target_Variable and Similar_var_#. ‘#’ stand for rank number. \n",
    "3. Similar_value_#: Similarity score for Similar_var_#. This score is calculated based on similarity between source variable definition and target variable definition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861164b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fuzzy_Match_Similarity:\n",
    "    def __init__(self,source_str_list,target_str_list,top_n,source_var_names,target_var_names):\n",
    "        self.source_str_list = source_str_list\n",
    "        self.target_str_list = target_str_list\n",
    "        self.n = top_n\n",
    "        self.target_name = source_var_names\n",
    "        self.findin_name = target_var_names\n",
    "    \n",
    "    def text_preprocessing(self,text):\n",
    "        # Lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        # Tokenization\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # Removing Punctuation\n",
    "        tokens = [token for token in tokens if token.isalpha()]\n",
    "\n",
    "        # Stopword Removal\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "        # Stemming\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "        # Join tokens back to a cleaned text\n",
    "        cleaned_text = ' '.join(tokens)\n",
    "\n",
    "        return cleaned_text\n",
    "    \n",
    "    \n",
    "    def result_df(self):\n",
    "        '''Build the empty dataframe for result'''\n",
    "        col_name = ['Target_Variable']\n",
    "        for i in range(self.n):\n",
    "            str_temp = 'similar_var_'+str(i)\n",
    "            sim_temp = 'similar_value_'+str(i)\n",
    "            col_name.append(str_temp)\n",
    "            col_name.append(sim_temp)\n",
    "        df = pd.DataFrame(columns=col_name)\n",
    "        return df\n",
    "    \n",
    "    def results(self,method):\n",
    "        result_df = self.result_df()\n",
    "        target_list = [self.text_preprocessing(text) for text in self.source_str_list]\n",
    "        findin_list = [self.text_preprocessing(text) for text in self.target_str_list]\n",
    "        for num1,text1 in enumerate(target_list):\n",
    "            sim_list_partial_ratio = []\n",
    "            row =[self.target_name[num1]]\n",
    "            for num2,text2 in enumerate(findin_list):\n",
    "                if method == 'token_sort_ratio':\n",
    "                    sim_num_partial_ratio = fuzz.token_sort_ratio(text1,text2)\n",
    "                elif method == 'ratio':\n",
    "                    sim_num_partial_ratio = fuzz.ratio(text1,text2)\n",
    "                elif method == 'partial_ratio':\n",
    "                    sim_num_partial_ratio = fuzz.partial_ratio(text1,text2)\n",
    "                elif method == 'token_set_ratio':\n",
    "                    sim_num_partial_ratio = fuzz.token_set_ratio(text1,text2)\n",
    "                elif method == 'partial_token_set_ratio':\n",
    "                    sim_num_partial_ratio = fuzz.partial_token_set_ratio(text1,text2)\n",
    "                elif method == 'partial_token_sort_ratio':\n",
    "                    sim_num_partial_ratio = fuzz.partial_token_sort_ratio(text1,text2)\n",
    "                else:\n",
    "                    print('Method Not Included')\n",
    "                \n",
    "                \n",
    "                sim_list_partial_ratio.append(sim_num_partial_ratio)\n",
    "            \n",
    "            sorted_list_with_positions = sorted(enumerate(sim_list_partial_ratio), key=lambda x: x[1],reverse=True)\n",
    "            top_n_list = sorted_list_with_positions[0:self.n]\n",
    "            for sets in top_n_list:\n",
    "                row.append(self.findin_name[sets[0]])\n",
    "                row.append(sets[1])\n",
    "            result_df.loc[len(result_df.index)] = row\n",
    "        return result_df\n",
    "            \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf53845",
   "metadata": {},
   "source": [
    "## Section 5. Semantic Textual Similarity Class\n",
    "Get Semantic textual similarity between two string using corresponding transformer model.\n",
    "Textual similarity is generated using cosine similarity. \n",
    "\n",
    "THe algorithm take 6 parameter as input: str_list1,str_list2,model,top_n,var_name_list1,var_name_list2\n",
    "1. str_list1: In this example, EU label/sheet_def/deriv_info strings in a list\n",
    "2. var_name_list1: In this example, EU variable name that corresponding to EU label/sheet_def/deriv_info strings in a list\n",
    "3. str_list2: In this example, JP label/sheet_def/deriv_info strings in a list\n",
    "4. var_name_list2: In this example, JP variable name that corresponding to EU label/sheet_def/deriv_info strings in a list\n",
    "5. model: The name of the transformer model you want to use. Option are : \"sentence-transformers/all-mpnet-base-v2\", \"intfloat/e5-large-v2\", \"sentence-transformers/all-MiniLM-L12-v2\"\n",
    "6. top_n: Number of top Similar variables you want to left in reuslt datasets. Must be smaller or equal to len(str_list2)\n",
    "\n",
    "Return format:Target_Variable, Similar_var_#, Similar_value_#\n",
    "1.\t Target_Variable: Variable names from the source data dictionary. Duplicate variable are dropped, variables with not label or definition are dropped.\n",
    "2.\tSimilar_var_#: Variable from the target data dictionary. Ranked based on the similarity score between target Target_Variable and Similar_var_#. ‘#’ stand for rank number. \n",
    "3.\tSimilar_value_#: Similarity score for Similar_var_#. This score is calculated based on similarity between source variable definition and target variable definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b944ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Semantic_Textual_Similarity:\n",
    "    def __init__(self,str_list1,str_list2,model,top_n,var_name_list1,var_name_list2):\n",
    "        self.str_list1 =str_list1\n",
    "        self.str_list2 =str_list2\n",
    "        self.name1 = var_name_list1\n",
    "        self.name2 = var_name_list2\n",
    "        self.model = model#\"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "        self.n = top_n\n",
    "    \n",
    "    def get_embeddings(self, strs_list):\n",
    "        '''Get embedding with transformer models'''\n",
    "        #Mean Pooling - Take attention mask into account for correct averaging\n",
    "        def mean_pooling(model_output, attention_mask):\n",
    "            token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "            input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "            sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "            sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "            return sum_embeddings / sum_mask\n",
    "        #Load AutoModel from huggingface model repository\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.model)\n",
    "        model = AutoModel.from_pretrained(self.model)\n",
    "        #Tokenize sentences\n",
    "        encoded_input = tokenizer(strs_list, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "        print('Encode finished')#max_length = 128\n",
    "        #Compute token embeddings\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input)\n",
    "            print('Output finished')\n",
    "        #Perform pooling. In this case, mean pooling\n",
    "        sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "        print('Pooling finished')\n",
    "        return sentence_embeddings\n",
    "    \n",
    "    def get_similarity(self,vector1,vector2):\n",
    "        '''Cosine similarity between two embedding vectors'''\n",
    "        return 1 - cosine(vector1, vector2)\n",
    "    \n",
    "    def result_df(self):\n",
    "        '''Build the empty dataframe for result'''\n",
    "        col_name = ['Target_Variable']\n",
    "        for i in range(self.n):\n",
    "            str_temp = 'similar_var_'+str(i)\n",
    "            sim_temp = 'similar_value_'+str(i)\n",
    "            col_name.append(str_temp)\n",
    "            col_name.append(sim_temp)\n",
    "        df = pd.DataFrame(columns=col_name)\n",
    "        return df\n",
    "        \n",
    "   \n",
    "                \n",
    "                \n",
    "    def result(self):\n",
    "        '''Main function that output the result data frame'''\n",
    "        result_df = self.result_df()\n",
    "        if self.model == 'sentence-transformers/stsb-roberta-large':\n",
    "            model = CrossEncoder('cross-encoder/stsb-roberta-large')\n",
    "            for n in range(len(self.str_list1)):\n",
    "                row = [self.name1[n]]\n",
    "                combinations = list(itertools.product([self.str_list1[n]], self.str_list2))\n",
    "                scores = model.predict(combinations)\n",
    "                sorted_indices = np.argsort(scores)\n",
    "\n",
    "                \n",
    "                top_indices = sorted_indices[-self.n:]  # Get the last N indices\n",
    "                selected_var = [self.name2[m] for m in top_indices]\n",
    "                top_values = scores[top_indices]    \n",
    "                for L in range(self.n):\n",
    "                    row.append(selected_var[self.n-L-1])\n",
    "                    row.append(top_values[self.n-L-1])\n",
    "                result_df.loc[len(result_df.index)] = row\n",
    "        \n",
    "        else:\n",
    "            if self.model in ['intfloat/multilingual-e5-large','intfloat/e5-large-v2']:\n",
    "                prefix = 'query: '\n",
    "                new_list1 = [prefix + item for item in self.str_list1]\n",
    "                new_list2 = [prefix + item for item in self.str_list2]\n",
    "                list1_embed = self.get_embeddings(new_list1)\n",
    "                list2_embed = self.get_embeddings(new_list2)\n",
    "            else:\n",
    "                list1_embed = self.get_embeddings(self.str_list1)\n",
    "                list2_embed = self.get_embeddings(self.str_list2)\n",
    "            result_df = self.result_df()\n",
    "            for num1,vec1 in enumerate(list1_embed):\n",
    "                sim_list = []\n",
    "                row =[self.name1[num1]]\n",
    "                for num2,vec2 in enumerate(list2_embed):\n",
    "                    sim_num = self.get_similarity(vec1,vec2)\n",
    "                    sim_list.append(sim_num)\n",
    "\n",
    "                sorted_list_with_positions = sorted(enumerate(sim_list), key=lambda x: x[1],reverse=True)\n",
    "                top_n_list = sorted_list_with_positions[0:self.n]\n",
    "                for sets in top_n_list:\n",
    "                    row.append(self.name2[sets[0]])\n",
    "                    row.append(sets[1])\n",
    "                result_df.loc[len(result_df.index)] = row\n",
    "        return result_df\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601b04f7",
   "metadata": {},
   "source": [
    "## Section 6. Specify Input columns and Run Algorithms\n",
    "#### Part 1. Specify which column to use for variables alignment algorithm (input columns).\n",
    "In this example, three parts can be used for similarity comparison.\n",
    "1. Sheet definition. Since both data dictionary has different data sheets for each category of dataset, we can alignment variables base in sheet definition.\n",
    "2. Keywords from deriviation rules. The deriviation rule can be NULL for some variables, so it's combined with label.\n",
    "3. Label. This is the definition of each variable, which is the main usage for variables' alignment.\n",
    "\n",
    "#### Part 2. Run different algorithms with different NLP methods.\n",
    "Three sets of options can be run for similarity: Sheet_Def, label_keywords, Label_CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3742ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1\n",
    "#top_n Number of top Similar variables you want to left in reuslt datasets. Must be smaller or equal to len(str_list2)\n",
    "top_n = len(JP_all['Label'])\n",
    "\n",
    "#If we want to use sheet definition for similarity comparison criterion, these set should be use as input\n",
    "#Source_Str1 = list(EU_all['Sheet_Def'].astype(str))\n",
    "#Target_Str2 = list(JP_all['Sheet_Def'].astype(str))\n",
    "\n",
    "#If we want to use deriviation rules for similarity comparison criterion, these set should be use as input\n",
    "#Source_Str1 = list(EU_all['label_keywords'].astype(str))\n",
    "#Target_Str2 = list(JP_all['label_keywords'].astype(str))\n",
    "\n",
    "#If we want to use labels for similarity comparison criterion, these set should be use as input\n",
    "Source_Str1 = list(EU_all['Label_CRF'].astype(str))\n",
    "Target_Str2 = list(JP_all['Label'].astype(str))\n",
    "\n",
    "#variable name\n",
    "source_name1 = list(EU_all['Variable'])\n",
    "target_name2 = list(JP_all['Variable'])\n",
    "\n",
    "#Truth map\n",
    "Truth_Map = Truth_table[['Variable_EU','Variable_JP']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a91e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Part 2\n",
    "#Fuzzy Match with partial token sort ratio\n",
    "\n",
    "fuzzy1  = Fuzzy_Match_Similarity(Source_Str1,Target_Str2,top_n,source_name1,target_name2)\n",
    "partial_token_sort_ratio_df = fuzzy1.results('partial_token_sort_ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171fa3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Fuzzy Match with token sort ratio\n",
    "fuzzy1 = Fuzzy_Match_Similarity(Source_Str1,Target_Str2,top_n,source_name1,target_name2)\n",
    "token_sort_ratio_df = fuzzy1.results('token_sort_ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3c0205",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Fuzzy Match with ratio\n",
    "fuzzy1 = Fuzzy_Match_Similarity(Source_Str1,Target_Str2,top_n,source_name1,target_name2)\n",
    "ratio_df = fuzzy1.results('ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005f72af",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Fuzzy Match with partial token set ratio\n",
    "\n",
    "fuzzy1 = Fuzzy_Match_Similarity(Source_Str1,Target_Str2,top_n,source_name1,target_name2)\n",
    "token_set_ratio_df = fuzzy1.results('token_set_ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb5e207",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Semantic similarity with sentence-transformers/all-mpnet-base-v2 model\n",
    "\n",
    "STS7 = Semantic_Textual_Similarity(Source_Str1,Target_Str2,\"sentence-transformers/all-mpnet-base-v2\",top_n,source_name1,target_name2)\n",
    "all_mpnet_base_v2_df = STS7.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb93de59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Semantic similarity with \"intfloat/e5-large-v2\" model\n",
    "STS8 = Semantic_Textual_Similarity(Source_Str1,Target_Str2,\"intfloat/e5-large-v2\",top_n,source_name1,target_name2)\n",
    "e5_large_v2_df = STS8.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51403843",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Semantic similarity with \"sentence-transformers/all-MiniLM-L12-v2\" model\n",
    "STS10 = Semantic_Textual_Similarity(Source_Str1,Target_Str2,\"sentence-transformers/all-MiniLM-L12-v2\",top_n,source_name1,target_name2)\n",
    "All_MiniLM_L12_v2_df = STS10.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65557b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Semantic similarity with \"FremyCompany/BioLORD-2023\" model\n",
    "STS11 = Semantic_Textual_Similarity(Source_Str1,Target_Str2,\"FremyCompany/BioLORD-2023\",top_n,source_name1,target_name2)\n",
    "Biolord2023_df = STS11.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60425b6d",
   "metadata": {},
   "source": [
    "## Section 7. Algorithm Evaluation with Truth Map\n",
    "Three Rank base methods are used for evaluation. The manual alignment truth map is only used in this evaluation process.\n",
    "Detail information can be found in here:\n",
    "https://towardsdatascience.com/ranking-evaluation-metrics-for-recommender-systems-263d0a66ef54\n",
    "1. Hit ratio\n",
    "2. Mean reciprocal rank\n",
    "3. Average precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc5d2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = len(JP_all['Label'])\n",
    "#Get hardness of variable base on mean_reciprocal_rank\n",
    "EU_reciprocal_rank = {}\n",
    "\n",
    "class Rank_base_evaluation:\n",
    "    def __init__(self,truth_map,result_df,Target_var_col_name_Truth,Findin_var_col_name_Truth):\n",
    "        '''Truth map is all the pairs of manual aligned variables '''\n",
    "        self.truth = truth_map\n",
    "        self.Target_var_col_name_Truth =Target_var_col_name_Truth\n",
    "        self.Findin_var_col_name_Truth = Findin_var_col_name_Truth\n",
    "        self.result_df = result_df\n",
    "        \n",
    "        \n",
    "    def Make_dict(self):\n",
    "        '''Convert the turth map into dictionary {'Var1':['Var2','Var3']...}\n",
    "        Conver the result_df into dictionary{'Var1':[rank1,rank2,rank3...]}'''\n",
    "        global top_n\n",
    "        grouped_truth = self.truth.groupby(self.Target_var_col_name_Truth)[self.Findin_var_col_name_Truth].apply(list).reset_index()\n",
    "        Truth_dict = dict(zip(grouped_truth[self.Target_var_col_name_Truth], grouped_truth[self.Findin_var_col_name_Truth]))\n",
    "        clo_names = []\n",
    "        clo_value = []\n",
    "        for k in range(top_n):\n",
    "            name = 'similar_var_'+str(k)\n",
    "            name_value =  'similar_value_'+str(k)\n",
    "            clo_names.append(name)\n",
    "            clo_value.append(name_value)\n",
    "        \n",
    "        Result_dict = {}\n",
    "        Result_dict_value = {}\n",
    "        Result_dict_score = {}\n",
    "        \n",
    "        for j in range(len(self.result_df)):\n",
    "            key = self.result_df['Target_Variable'].iloc[j]\n",
    "            items = list(self.result_df[clo_names].iloc[j].values)\n",
    "            value = list(self.result_df[clo_value].iloc[j].values)\n",
    "            rank = [sorted(value,reverse=True).index(x) +1 for x in value]\n",
    "            Result_dict[key] = items\n",
    "            Result_dict_value[key] = rank\n",
    "            Result_dict_score[key] = value\n",
    "        \n",
    "        return Truth_dict,Result_dict,Result_dict_value,Result_dict_score\n",
    "    \n",
    "    def hit_ratio(self, k):\n",
    "        global top_n\n",
    "        \"\"\"\n",
    "        Calculate the Hit Ratio (HR) at a given rank k.\n",
    "        \"\"\"\n",
    "        Truth_dict,Result_dict,Result_dict_value,Result_dict_score =self.Make_dict()\n",
    "        if k<=top_n:\n",
    "            total_var = len(Truth_dict)#08012023Result_dict  Truth table len = 30/ hit is at most 30\n",
    "        \n",
    "            correct_predictions = 0\n",
    "\n",
    "            for Target_var, ranked_items in Result_dict.items():\n",
    "                truth_items = Truth_dict.get(Target_var, set())\n",
    "                top_k_items = ranked_items[:k]\n",
    "                if any(item in truth_items for item in top_k_items):\n",
    "                    correct_predictions += 1\n",
    "            \n",
    "            \n",
    "            #print(f'Total of {correct_predictions} hit in {total_var} variabels')\n",
    "\n",
    "            return correct_predictions / total_var\n",
    "        else:\n",
    "            print(f\"Only top {top_n} similar variable is recorded. Narrow your evaluation rank k.\")\n",
    "            \n",
    "    \n",
    "    def mean_reciprocal_rank(self):\n",
    "        \"\"\"\n",
    "        Calculate the Mean Reciprocal Rank (MRR).\n",
    "        \n",
    "        \"\"\"\n",
    "        Truth_dict,Result_dict,Result_dict_value, Result_dict_score =self.Make_dict()\n",
    "        \n",
    "        total_var = len(Truth_dict) #Result_dict\n",
    "        reciprocal_ranks = []\n",
    "        reciprocal_ranks_var = []\n",
    "\n",
    "        for Target_var, ranked_items in Result_dict.items():\n",
    "            truth_items = Truth_dict.get(Target_var, set())\n",
    "            rank_list = Result_dict_value[Target_var]\n",
    "            for rank, item in enumerate(ranked_items):\n",
    "                if item in truth_items:\n",
    "                    rank_value = rank_list[rank]\n",
    "                    reciprocal_ranks.append(1 / rank_value)\n",
    "                    reciprocal_ranks_var.append(Target_var)\n",
    "                    #print(Target_var)\n",
    "                    break\n",
    "               \n",
    "\n",
    "        return reciprocal_ranks,reciprocal_ranks_var,sum(reciprocal_ranks) / total_var\n",
    "        \n",
    "        \n",
    "    def average_precision(self):\n",
    "        \"\"\"\n",
    "        Calculate the Average Precision for each query and return the mean of all queries' Average Precision.\n",
    "        \n",
    "        \"\"\"\n",
    "        Truth_dict,Result_dict,Result_dict_value,Result_dict_score =self.Make_dict()\n",
    "        total_var = len(Truth_dict) #Result_dict\n",
    "        average_precisions = []\n",
    "\n",
    "        for Target_var, ranked_items in Result_dict.items():\n",
    "            truth_items = Truth_dict.get(Target_var, set())\n",
    "            rank_list = Result_dict_value[Target_var]\n",
    "            num_correct_predictions = 0\n",
    "            precision_sum = 0.0\n",
    "\n",
    "            for rank, item in enumerate(ranked_items):\n",
    "                if item in truth_items:\n",
    "                    rank_value = rank_list[rank]\n",
    "                    num_correct_predictions += 1\n",
    "                    precision_sum += num_correct_predictions / rank_value\n",
    "\n",
    "            average_precisions.append(precision_sum / max(len(truth_items), 1))\n",
    "\n",
    "        return sum(average_precisions) / total_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58547d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get evaluation results\n",
    "Eva_STS9 = Rank_base_evaluation(Truth_Map,token_set_ratio_df,'Variable_EU','Variable_JP')\n",
    "Eva_STS12 = Rank_base_evaluation(Truth_Map,all_mpnet_base_v2_df,'Variable_EU','Variable_JP')\n",
    "Eva_STS13 = Rank_base_evaluation(Truth_Map,e5_large_v2_df,'Variable_EU','Variable_JP')\n",
    "Eva_STS16 = Rank_base_evaluation(Truth_Map,All_MiniLM_L12_v2_df,'Variable_EU','Variable_JP')\n",
    "Eva_STS19 = Rank_base_evaluation(Truth_Map,Biolord2023_df,'Variable_EU','Variable_JP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99736abe-9928-49eb-9143-963d2a25fa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visulizing Hit Ratio Graph\n",
    "plt.figure(figsize =(8, 8),dpi = 300)\n",
    "Eva_STS9_topn_hitrario = []\n",
    "Eva_STS12_topn_hitrario = []\n",
    "Eva_STS13_topn_hitrario = []\n",
    "Eva_STS16_topn_hitrario = []\n",
    "Eva_STS19_topn_hitrario = []\n",
    "x_values = []\n",
    "\n",
    "for n in range(1,31):\n",
    "    print(n)\n",
    "    Eva_STS9_topn_hitrario.append(Eva_STS9.hit_ratio(n))\n",
    "    Eva_STS12_topn_hitrario.append(Eva_STS12.hit_ratio(n))\n",
    "    Eva_STS13_topn_hitrario.append(Eva_STS13.hit_ratio(n))\n",
    "    Eva_STS16_topn_hitrario.append(Eva_STS16.hit_ratio(n))\n",
    "    Eva_STS19_topn_hitrario.append(Eva_STS19.hit_ratio(n))\n",
    "    x_values.append(n)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5f3d83-155e-4d0c-a6f2-d07a7ea94ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot points for hit ratio in top 1 to top 31\n",
    "plt.scatter(x_values, Eva_STS9_topn_hitrario, label='token_set_ratio', color='#8A2BE2', marker='s')\n",
    "plt.scatter(x_values, Eva_STS12_topn_hitrario, label='all_mpnet_base_v2', color='#7FFF00', marker='X')\n",
    "plt.scatter(x_values, Eva_STS13_topn_hitrario, label='e5_large_v2', color='#458B00', marker='D')\n",
    "plt.scatter(x_values, Eva_STS16_topn_hitrario, label='All_MiniLM_L12_v2_df', color='#8B8B00', marker='2')\n",
    "plt.scatter(x_values, Eva_STS19_topn_hitrario, label='Biolord2023_df', color='#88c999', marker='2')\n",
    "\n",
    "\n",
    "#Add lines between scatter\n",
    "plt.plot(x_values, Eva_STS12_topn_hitrario, color='red')\n",
    "plt.plot(x_values, Eva_STS13_topn_hitrario, color='red')\n",
    "plt.plot(x_values, Eva_STS16_topn_hitrario, color='red')\n",
    "plt.plot(x_values, Eva_STS19_topn_hitrario, color='red')\n",
    "plt.plot(x_values, Eva_STS9_topn_hitrario, color='blue')\n",
    "\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Truth Align in TOP 30')\n",
    "plt.ylabel('Hit Ratio')\n",
    "plt.title('Hit Ratio for Truth Aligned showed up in TOP 30 Similar')\n",
    "\n",
    "# Add legend\n",
    "plt.legend(loc='lower right', title='Models')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2e89ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visulizing mean_reciprocal_rank and average_precision Graph\n",
    "\n",
    "plt.figure(figsize =(4, 4),dpi = 300)\n",
    "columns = ['token_set_ratio','all_mpnet_base_v2','e5_large_v2','All_MiniLM_L12_v2','Biolord2023']\n",
    "mean_reciprocal_rank = []\n",
    "average_precision = []\n",
    "for i in [Eva_STS9,Eva_STS12,Eva_STS13,Eva_STS16,Eva_STS19]:\n",
    "    mean_reciprocal_rank.append(i.mean_reciprocal_rank()[2])\n",
    "    average_precision.append(i.average_precision())\n",
    "data = {'mean_reciprocal_rank':mean_reciprocal_rank,'mean_average_precision':average_precision}\n",
    "Eva_df = pd.DataFrame(data = data,index = columns)\n",
    "#Eva_df\n",
    "\n",
    "plt.scatter(Eva_df['mean_reciprocal_rank'], Eva_df['mean_average_precision'])\n",
    "texts = []\n",
    "# Add index labels to the points\n",
    "for i, (x, y) in enumerate(zip(Eva_df['mean_reciprocal_rank'], Eva_df['mean_average_precision'])):\n",
    "    texts.append(plt.text(x, y, f\"{Eva_df.index[i]}\"))\n",
    "    #plt.text(x, y, f\"{Eva_df.index[i]}\", ha='right', va='bottom')\n",
    "    \n",
    "#fix overlapping annotations / text\n",
    "adjust_text(texts, only_move={'points':'y', 'texts':'y'}, arrowprops=dict(arrowstyle=\"->\", color='r', lw=0.5))\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('mean_reciprocal_rank')\n",
    "plt.ylabel('mean_average_precision')\n",
    "plt.title('Models Performance')\n",
    "\n",
    "# Show the plot\n",
    "#plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38eb4585-2166-42fb-a0d5-9863ef047db6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
