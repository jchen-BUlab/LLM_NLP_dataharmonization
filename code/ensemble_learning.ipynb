{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59d7fdbe-bc1e-458f-8d4e-e53d12551822",
   "metadata": {},
   "source": [
    "## A Large Language Model-based tool to facilitate data harmonization: Random Forest model used to align variables across cohort studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4582b3c0-383b-4a7e-8d33-0e04d695aaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#****************************************\n",
    "# MIT License\n",
    "# Copyright (c) 2025 Zexu Li, Jinying Chen\n",
    "#  \n",
    "# author(s): Zexu Li, Jinying Chen, Boston University Chobanian & Avedisian School of Medicine\n",
    "# date: 2025-7-7\n",
    "# ver: 1.0\n",
    "# \n",
    "# This code was written to support data analysis for the Data Harmonization Using Natural Language \n",
    "# Processing (NLP harmonization) project and the 2025 paper published in PLOS One.\n",
    "# The code is for research use only, and is provided as it is.\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153ee88b-8161-4215-9958-a9e644a6619a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3f4be1-72c0-48ad-83d5-afecc274a834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load input data for ML model\n",
    "datadir = \"[path to input data]\"\n",
    "Final_df_ML = pd.read_csv(datadir + 'ML_dataset_021825_v3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b6361c-3db0-4b9d-95b6-b77cf5e6bb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_df_ML.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc941942",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_df_ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a44b32-4ff8-4bc9-a2fa-716baf183c2b",
   "metadata": {},
   "source": [
    "# Example code for runing a single trial in the ML experiments\n",
    "(Grid search of 50 trials presented in Grid_search_and_evaluation_50_trials.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3116a2f6-3e01-4dd6-9a35-eb51e0569e84",
   "metadata": {},
   "source": [
    "## Split Final dataset to test/train/valid base on source variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953e4ec6-cf17-4226-9a48-1f00cdb5dc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify unique values in the 'source' column\n",
    "unique_sources = Final_df_ML['Source'].unique()\n",
    "\n",
    "# Choose a subset of unique sources for training and testing\n",
    "train_sources, test_sources = train_test_split(unique_sources, test_size=0.2, random_state=42)\n",
    "\n",
    "train_sources_small, validation_sources = train_test_split(train_sources, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Filter the original DataFrame to create training and testing sets\n",
    "train = Final_df_ML[Final_df_ML['Source'].isin(train_sources_small)]\n",
    "Ori_train = Final_df_ML[Final_df_ML['Source'].isin(train_sources_small)]\n",
    "validation =  Final_df_ML[Final_df_ML['Source'].isin(validation_sources)]\n",
    "Ori_validation = Final_df_ML[Final_df_ML['Source'].isin(validation_sources)]\n",
    "test = Final_df_ML[Final_df_ML['Source'].isin(test_sources)]\n",
    "Ori_test = Final_df_ML[Final_df_ML['Source'].isin(test_sources)]\n",
    "\n",
    "global global_validation\n",
    "global_validation = Ori_validation\n",
    "\n",
    "\n",
    "global global_test\n",
    "global_test = Ori_test\n",
    "\n",
    "global global_train\n",
    "global_train = Ori_train\n",
    "\n",
    "\n",
    "train= train[['miniLM_on_label', 'e5_on_label', 'mpnet_on_label', 'fuzzy_on_label','biolord_on_label',\n",
    "                        'miniLM_on_label_key', 'e5_on_label_key', 'mpnet_on_label_key', 'fuzzy_on_label_key','biolord_on_label_key',\n",
    "                        'miniLM_on_sheet', 'e5_on_sheet', 'mpnet_on_sheet', 'fuzzy_on_sheet','biolord_on_sheet',\n",
    "                       'deriv_info_null_EU','deriv_info_len_EU','Label_len_EU',\n",
    "                       'deriv_info_null_JP','deriv_info_len_JP','Label_len_JP','Mapping_result']]\n",
    "\n",
    "validation= validation[['miniLM_on_label', 'e5_on_label', 'mpnet_on_label', 'fuzzy_on_label','biolord_on_label',\n",
    "                        'miniLM_on_label_key', 'e5_on_label_key', 'mpnet_on_label_key', 'fuzzy_on_label_key','biolord_on_label_key',\n",
    "                        'miniLM_on_sheet', 'e5_on_sheet', 'mpnet_on_sheet', 'fuzzy_on_sheet','biolord_on_sheet',\n",
    "                       'deriv_info_null_EU','deriv_info_len_EU','Label_len_EU',\n",
    "                       'deriv_info_null_JP','deriv_info_len_JP','Label_len_JP','Mapping_result']]\n",
    "\n",
    "test= test[['miniLM_on_label', 'e5_on_label', 'mpnet_on_label', 'fuzzy_on_label','biolord_on_label',\n",
    "                        'miniLM_on_label_key', 'e5_on_label_key', 'mpnet_on_label_key', 'fuzzy_on_label_key','biolord_on_label_key',\n",
    "                        'miniLM_on_sheet', 'e5_on_sheet', 'mpnet_on_sheet', 'fuzzy_on_sheet','biolord_on_sheet',\n",
    "                       'deriv_info_null_EU','deriv_info_len_EU','Label_len_EU',\n",
    "                       'deriv_info_null_JP','deriv_info_len_JP','Label_len_JP','Mapping_result']]\n",
    "\n",
    "\n",
    "\n",
    "X_validation = validation.drop('Mapping_result',axis = 1)\n",
    "#X = np.array(X)\n",
    "y_validation = validation[['Mapping_result']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a629353f-f9d9-4914-9c61-15e9a3df7d45",
   "metadata": {},
   "source": [
    "## Downsampling on the negative pairs (1:200 ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6521ac5a-89c7-4084-b880-204187795c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "#200 ratio setup\n",
    "def select_true_and_false(df_source):\n",
    "    true_values = df_source[df_source['Mapping_result'] == 1]\n",
    "    false_values = df_source[df_source['Mapping_result'] == 0].sample(n=200*len(true_values), random_state=42)\n",
    "    return pd.concat([true_values, false_values])\n",
    "\n",
    "#testing RF model with default parameters\n",
    "#repeating model training/testing for 10 times\n",
    "'''\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "\n",
    "    final_train_set = Ori_train.groupby('Source').apply(select_true_and_false).reset_index(drop=True)\n",
    "    final_train_set = final_train_set[['miniLM_on_label', 'e5_on_label', 'mpnet_on_label', 'fuzzy_on_label','biolord_on_label',\n",
    "                        'miniLM_on_label_key', 'e5_on_label_key', 'mpnet_on_label_key', 'fuzzy_on_label_key','biolord_on_label_key',\n",
    "                        'miniLM_on_sheet', 'e5_on_sheet', 'mpnet_on_sheet', 'fuzzy_on_sheet','biolord_on_sheet',\n",
    "                       'deriv_info_null_EU','deriv_info_len_EU','Label_len_EU',\n",
    "                       'deriv_info_null_JP','deriv_info_len_JP','Label_len_JP','Mapping_result']]\n",
    "\n",
    "    \n",
    "    X_train = final_train_set.drop('Mapping_result',axis = 1)\n",
    "    y_train = final_train_set[['Mapping_result']].values.ravel()\n",
    "\n",
    "\n",
    "    RF_model =  RandomForestClassifier()\n",
    "\n",
    "    RF_model.fit(X_train,y_train)\n",
    "    y_pred = RF_model.predict(X_validation)\n",
    "    accuracy = accuracy_score(y_validation, y_pred)\n",
    "\n",
    "    report = classification_report(y_validation, y_pred)\n",
    "    #print(f'{n} random sample are selected')\n",
    "    print(report)\n",
    "    probability_estimates = RF_model.predict_proba(X_validation)\n",
    "    \n",
    "    temp_df = Ori_validation.copy()\n",
    "    temp_df['probability'] = RF_model.predict_proba(X_validation)[:,1]\n",
    "    temp_df['rank'] = temp_df.groupby('Source')['probability'].rank(ascending=False)\n",
    "    positive = temp_df[temp_df['Mapping_result'] == 1]\n",
    "    top5 = len(positive[positive['rank'] <=5])/ len(positive)\n",
    "    top10 = len(positive[positive['rank'] <=10])/ len(positive)\n",
    "    top20 = len(positive[positive['rank'] <=20])/ len(positive)\n",
    "    top30 = len(positive[positive['rank'] <=30])/ len(positive)\n",
    "    \n",
    "    print(f'Hit rate: {top5},{top10},{top20},{top30} ')\n",
    "    \n",
    "\n",
    "    # Print the probability estimates for the first few samples\n",
    "    print(\"Probability Estimates:\")\n",
    "    print(probability_estimates[:10])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa677b58-df8a-4480-b0ae-f8bc5795974e",
   "metadata": {},
   "source": [
    "## Grid search for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4a0e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_set = Ori_train.groupby('Source').apply(select_true_and_false).reset_index(drop=True)\n",
    "global global_train\n",
    "global_train = final_train_set\n",
    "final_train_set = final_train_set[['miniLM_on_label', 'e5_on_label', 'mpnet_on_label', 'fuzzy_on_label','biolord_on_label',\n",
    "                        'miniLM_on_label_key', 'e5_on_label_key', 'mpnet_on_label_key', 'fuzzy_on_label_key','biolord_on_label_key',\n",
    "                        'miniLM_on_sheet', 'e5_on_sheet', 'mpnet_on_sheet', 'fuzzy_on_sheet','biolord_on_sheet',\n",
    "                       'deriv_info_null_EU','deriv_info_len_EU','Label_len_EU',\n",
    "                       'deriv_info_null_JP','deriv_info_len_JP','Label_len_JP','Mapping_result']]\n",
    "X_train = final_train_set.drop('Mapping_result',axis = 1)\n",
    "y_train = final_train_set[['Mapping_result']].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe92f70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = Final_df_ML[Final_df_ML['Source'].isin(train_sources)]\n",
    "test = Final_df_ML[Final_df_ML['Source'].isin(test_sources)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f4c9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_sublists(lst, k):\n",
    "    \"\"\"\n",
    "    Split a list into k sublists without overlapping.\n",
    "\n",
    "    Parameters:\n",
    "    - lst: List to be split.\n",
    "    - k: Number of sublists.\n",
    "\n",
    "    Returns:\n",
    "    A list of k sublists.\n",
    "    \"\"\"\n",
    "    n = len(lst)\n",
    "    sublist_size = n // k\n",
    "    remainder = n % k\n",
    "\n",
    "    sublists = []\n",
    "    start = 0\n",
    "\n",
    "    for i in range(k):\n",
    "        sublist_length = sublist_size + (1 if i < remainder else 0)\n",
    "        sublists.append(lst[start:start + sublist_length])\n",
    "        start += sublist_length\n",
    "\n",
    "    return sublists\n",
    "\n",
    "\n",
    "split_train_source = split_into_sublists(train_sources, 5)\n",
    "#print(split_train_source)\n",
    "for i in split_train_source:\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d9d32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score,f1_score\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc, make_scorer, recall_score, accuracy_score, precision_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "param_grid = {\n",
    "        'n_estimators': [50, 200,500,900],\n",
    "        'max_depth': [2, 10, 15, 20], \n",
    "        'min_samples_split': [2, 5, 10, 15],\n",
    "        'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "        'max_features':['sqrt', None]\n",
    "        \n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0370288c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909e67af",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_HR = -1\n",
    "best_MRR = -1\n",
    "n = 0\n",
    "mean_HR_list = []\n",
    "mean_MRR_list = []\n",
    "para_list = []\n",
    "for g in ParameterGrid(param_grid):\n",
    "        #rf = RandomForestClassifier(random_state=42)\n",
    "        #rf.set_params(**g)\n",
    "        HR_list = []\n",
    "        MRR_list = []\n",
    "        for split in split_train_source:\n",
    "            rf = RandomForestClassifier(random_state=42)\n",
    "            rf.set_params(**g)\n",
    "            validation_data = train[train['Source'].isin(split)]\n",
    "            train_data = train[~train['Source'].isin(split)]\n",
    "            \n",
    "            final_train_set = train_data.groupby('Source').apply(select_true_and_false).reset_index(drop=True)\n",
    "            final_validation_set = validation_data.copy()\n",
    "            \n",
    "\n",
    "            train_dataonly = final_train_set[['miniLM_on_label', 'e5_on_label', 'mpnet_on_label', 'fuzzy_on_label','biolord_on_label',\n",
    "                        'miniLM_on_label_key', 'e5_on_label_key', 'mpnet_on_label_key', 'fuzzy_on_label_key','biolord_on_label_key',\n",
    "                        'miniLM_on_sheet', 'e5_on_sheet', 'mpnet_on_sheet', 'fuzzy_on_sheet','biolord_on_sheet',\n",
    "                       'deriv_info_null_EU','deriv_info_len_EU','Label_len_EU',\n",
    "                       'deriv_info_null_JP','deriv_info_len_JP','Label_len_JP','Mapping_result']]\n",
    "            validation_dataonly = final_validation_set[['miniLM_on_label', 'e5_on_label', 'mpnet_on_label', 'fuzzy_on_label','biolord_on_label',\n",
    "                        'miniLM_on_label_key', 'e5_on_label_key', 'mpnet_on_label_key', 'fuzzy_on_label_key','biolord_on_label_key',\n",
    "                        'miniLM_on_sheet', 'e5_on_sheet', 'mpnet_on_sheet', 'fuzzy_on_sheet','biolord_on_sheet',\n",
    "                       'deriv_info_null_EU','deriv_info_len_EU','Label_len_EU',\n",
    "                       'deriv_info_null_JP','deriv_info_len_JP','Label_len_JP','Mapping_result']]\n",
    "\n",
    "            X_train = train_dataonly.drop('Mapping_result',axis = 1)\n",
    "            y_train = train_dataonly[['Mapping_result']].values.ravel()\n",
    "            X_validation = validation_dataonly.drop('Mapping_result',axis = 1)\n",
    "            y_validation = validation_dataonly[['Mapping_result']].values.ravel()\n",
    "            rf.fit(X_train,y_train)\n",
    "            y_pred = rf.predict(X_validation)\n",
    "            y_pred_proba = rf.predict_proba(X_validation)[:, 1]\n",
    "            final_validation_set['probability'] = y_pred_proba\n",
    "            final_validation_set['rank'] = final_validation_set.groupby('Source')['probability'].rank(ascending=False)\n",
    "            positive = final_validation_set[final_validation_set['Mapping_result'] == 1]\n",
    "            \n",
    "            top30_HR = len(positive[positive['rank'] <=30])/ len(positive)\n",
    "            HR_list.append(top30_HR)\n",
    "            max_ranks = positive.groupby('Source')['rank'].idxmin()\n",
    "            result_df = positive.loc[max_ranks]\n",
    "            result_df['MRR'] = 1/result_df['rank']\n",
    "\n",
    "\n",
    "            MRR = sum(result_df['MRR'])/ len(result_df)\n",
    "            MRR_list.append(MRR)\n",
    "        mean_HR = sum(HR_list) / len(HR_list)\n",
    "        mean_MRR = sum(MRR_list) / len(MRR_list)\n",
    "        if mean_HR > best_HR:\n",
    "            best_HR = mean_HR\n",
    "            best_grid_HR = g\n",
    "        if mean_MRR > best_MRR:\n",
    "            best_MRR = mean_MRR\n",
    "            best_grid_MRR = g\n",
    "        n+=1\n",
    "        mean_HR_list.append(mean_HR)   \n",
    "        mean_MRR_list.append(mean_MRR)\n",
    "        para_list.append(g)\n",
    "        print(f'Step {n} complete: model:{rf}, HR: {mean_HR}, MRR: {mean_MRR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb01a16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.set_params(**best_grid_HR)\n",
    "train = Final_df_ML[Final_df_ML['Source'].isin(train_sources)]\n",
    "test = Final_df_ML[Final_df_ML['Source'].isin(test_sources)]\n",
    "\n",
    "\n",
    "test_copy = test.copy()\n",
    "\n",
    "test_data = test[['miniLM_on_label', 'e5_on_label', 'mpnet_on_label', 'fuzzy_on_label','biolord_on_label',\n",
    "                        'miniLM_on_label_key', 'e5_on_label_key', 'mpnet_on_label_key', 'fuzzy_on_label_key','biolord_on_label_key',\n",
    "                        'miniLM_on_sheet', 'e5_on_sheet', 'mpnet_on_sheet', 'fuzzy_on_sheet','biolord_on_sheet',\n",
    "                       'deriv_info_null_EU','deriv_info_len_EU','Label_len_EU',\n",
    "                       'deriv_info_null_JP','deriv_info_len_JP','Label_len_JP','Mapping_result']]\n",
    "final_train_set = train.groupby('Source').apply(select_true_and_false).reset_index(drop=True)\n",
    "\n",
    "train_dataonly = final_train_set[['miniLM_on_label', 'e5_on_label', 'mpnet_on_label', 'fuzzy_on_label','biolord_on_label',\n",
    "                        'miniLM_on_label_key', 'e5_on_label_key', 'mpnet_on_label_key', 'fuzzy_on_label_key','biolord_on_label_key',\n",
    "                        'miniLM_on_sheet', 'e5_on_sheet', 'mpnet_on_sheet', 'fuzzy_on_sheet','biolord_on_sheet',\n",
    "                       'deriv_info_null_EU','deriv_info_len_EU','Label_len_EU',\n",
    "                       'deriv_info_null_JP','deriv_info_len_JP','Label_len_JP','Mapping_result']]\n",
    "\n",
    "    \n",
    "\n",
    "X_test = test_data.drop('Mapping_result',axis = 1)\n",
    "y_test = test_data[['Mapping_result']].values.ravel()\n",
    "\n",
    "X_train = train_dataonly.drop('Mapping_result',axis = 1)\n",
    "y_train = train_dataonly[['Mapping_result']].values.ravel()\n",
    "\n",
    "\n",
    "rf.fit(X_train,y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "y_pred_proba = rf.predict_proba(X_test)[:, 1]\n",
    "test_copy['probability'] = y_pred_proba\n",
    "test_copy['rank'] = test_copy.groupby('Source')['probability'].rank(ascending=False)\n",
    "positive = test_copy[test_copy['Mapping_result'] == 1]\n",
    "\n",
    "top30_HR = len(positive[positive['rank'] <=30])/ len(positive)\n",
    "top20_HR = len(positive[positive['rank'] <=20])/ len(positive)\n",
    "top10_HR = len(positive[positive['rank'] <=10])/ len(positive)\n",
    "top5_HR = len(positive[positive['rank'] <=5])/ len(positive)\n",
    "\n",
    "max_ranks = positive.groupby('Source')['rank'].idxmin()\n",
    "result_df = positive.loc[max_ranks]\n",
    "result_df['MRR'] = 1/result_df['rank']\n",
    "MRR = sum(result_df['MRR'])/ len(result_df)\n",
    "print(top30_HR,top20_HR,top10_HR,top5_HR,MRR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523cd631-b166-4d47-96ec-1eb3bce1944b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
